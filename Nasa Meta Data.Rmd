---
title: "Nasa Meta Data"
author: "Richard G. Gardiner"
date: "12/5/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Intro

Disclosure: Most of the information below comes from *Text Mining with R* by David Robinson.  

NASA has tens of thousands of datasets that cover everything from earth science to aeospace enginerring.  We can use metadata for these datasets to understand how they all connect.  The metadata includes the name of the dataset, description, which organization(s) is responsible for the dataset, and keywords.  The metadata is publically available only in JSON format.

Using techniques like tf-idf and topic modeling, we will explose the connections between the different datasets.

# How the data is organized

The first we want to do is download the JSON file and look at the names of what is stored in the metadata

```{r, cache = TRUE}
library(jsonlite)

metadata <- fromJSON("https://data.nasa.gov/data.json")
names(metadata$dataset)
```


We see here that we could extract a lot of information from who publishes each dataset to what license they are released under.  It seems likely that the title, description, and keywords for each dataset may be the best for connecting the datasets.  Let's check them out:

```{r}
class(metadata$dataset$title)
class(metadata$dataset$description)
class(metadata$dataset$keyword)
```

The title and description are stored as characters while the keyword is stored as a list.

## Wrangling and Tidying the data

Now we can set up tidy data frames for title, description, and keyword, keeping the data ids for each so we can connect them if necessary for later analysis

```{r}
library(tidyverse)

nasa_title <- data_frame(id = metadata$dataset$identifier, 
                         title = metadata$dataset$title)

unique <- unique(nasa_title$id)
```


The book has a different way to capture the id, but it wasn't working and this is the best idea I had for this.  It appears that there may be a little bit of overlap, but that is going to have to do for now (the book also mentions overlap).


```{r}
nasa_description <- data_frame(id = metadata$dataset$identifier,
                               desc = metadata$dataset$description)

nasa_description %>%
  select(desc) %>%
  sample_n(5)
```

Lastly, we will want to build a tidy data frame for the keywords.  For this one, we need to use `unnest()` from tidyr, because they are in a list-column.

```{r}
nasa_keyword <- data_frame(id = metadata$dataset$identifier,
                          keyword = metadata$dataset$keyword) %>%
  unnest(keyword)

nasa_keyword
```

This is a tidy data frame where each row is a keyword.  This means there are multiple rows for most of the datasets.

Now we can use the `unnest_tokens()` function for the title and description fiedls so we can start doing text analysis.  We will also remove stop from the descriptions and titles, but not the keywords because they are short-human assigned keywords.

```{r}
library(tidytext)

nasa_title <- nasa_title %>%
  unnest_tokens(word, title) %>%
  anti_join(stop_words)

nasa_description <- nasa_description %>%
  unnest_tokens(word, desc) %>%
  anti_join(stop_words)
```


Now they are in a tidy text format with one token (word) per row:

```{r}
nasa_title
```


```{r}
nasa_description
```


## Initial Analysis


Let's get a list of the most common words in the dataset titles using dplyr.

```{r}
nasa_title %>%
  count(word, sort = TRUE)
```

How about descriptions?

```{r}
nasa_description %>%
  count(word, sort = TRUE)
```

Let's remove some of the words that are used frequently, but are not meaningful to most of us.  This can be done using a custom stop word then doing an anti join.

```{r}
my_stop_words <- data_frame(word = c(as.character(1:5),
                                 "ii", "v1.0", "l2", "l3", "1"))

nasa_title <- nasa_title %>%
  anti_join(my_stop_words)

nasa_description <- nasa_description %>%
  anti_join(my_stop_words)
```

What are the most common keywords?

```{r}
nasa_keyword %>%
  count(keyword, sort = TRUE)
```


This looks pretty good, but we might want to change all of our text to upper or lower case to get rid of duplicates when we do analysis:

```{r}
nasa_keyword <- nasa_keyword %>%
  mutate(keyword = toupper(keyword))

```

# Word co-ocurrences and correlations

Lets examine which words commonly occur together in the title,s descriptions, and keywords of NASA datasets.  This may help us see which datasets are related to each other.

## Networks of description and title words

We can use `pairwise_count()` from widyr package to count how many time each pair of words occur together in a title or description field.

```{r}
library(widyr)

title_word_pairs <- nasa_title %>%
  widyr::pairwise_count(word, id, sort = TRUE, upper = FALSE) 

title_word_pairs
```

These are pairs of words that occur together most often in title fields.  We see a lot of "phase" in this top 10.  Now let's do the same with descriptions:

```{r}
desc_word_pairs <- nasa_description %>% 
  widyr::pairwise_count(word, id, sort = TRUE, upper = FALSE)

desc_word_pairs
```

As with titles, the word "phase" and "system" are pretty common.


Let's plot networks of these cooccurring words so we can see these relationships better.  We will use the ggraph package to visualize the networks:

```{r}
library(ggplot2)
library(igraph)
library(ggraph)


set.seed(1234)
title_word_pairs %>%
  filter(n >= 200) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
  geom_node_point(size = 3) +
  geom_node_text(aes(label = name), repel = TRUE,
                 point.padding = unit(0.2, "lines")) +
  theme_void()

```

So phase clearly is the key part of this.  Perhaps it is actually too important, but we can deal with that later.  It might be more helpful to do tf-idf as a metric to find characteristic words for each description field.  Let's look at the description fields:

```{r}
set.seed(1234)
desc_word_pairs %>%
  filter(n >= 1200) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "blue") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE,
                 point.padding = unit(0.2, "lines")) +
  theme_void()
```


This is showing a more complex network (though I had to play with the filter).

Now let's do networks of keywords:

```{r}
keyword_pairs <- nasa_keyword %>%
  pairwise_count(keyword, id, sort = TRUE, upper = FALSE)

keyword_pairs
```


```{r}
set.seed(1234)
keyword_pairs %>%
  filter(n >= 200) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "red") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE,
                 point.padding = unit(0.2, "lines")) +
  theme_void()

```


NOTE: these are the most common keywords occuring together, but they are also in general the most common keywords.

We can also do pairwise correlation with keywords to see which keywords that are more likely to occur together than other keywords.

```{r}
keyword_cor <- nasa_keyword %>%
  group_by(keyword) %>%
  filter(n() >= 50) %>%
  pairwise_cor(keyword, id, sort = TRUE, upper = FALSE)

keyword_cor
```

Note that we have a few that always occur together (correlation of 1).  This means that they are redundant keywords.  We can visualize a network of keyword correlations just like the co-occurences.

```{r}
set.seed(1234)
keyword_cor %>%
  filter(correlation > .7) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation, edge_width = correlation), edge_colour = "green") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE,
                 point.padding = unit(0.2, "lines")) +
  theme_void()

```

This is rather interesting and different than the co=occurences network.  The main difference is that this graph is asking about which keyword pairs occur more oten together than with other words. In this network, there are a lot of clusters that are independent of each other.  This can be extracted for further analysis.



# Calculating tf-idf for the description fields

The results of the pairwise count for the descriptions showed that there are just a few words that dominate the fields.  In this instance, using tf-idf would be great to find words that are espeically important to a document within a collection of documents.  For this example, we will consider each description field a "document" and the entire dataframe as the colelction (corpus) of documents.  Since we have already used the `unnest_tokens()` function, we only need to use the `bind_tf_idf()` function:

```{r}
desc_tf_idf <- nasa_description %>%
  count(id, word, sort = TRUE) %>%
  ungroup() %>%
  bind_tf_idf(word, id, n)

```


```{r}
desc_tf_idf %>%
  arrange(-tf_idf)

```

These are teh mostimportant words in teh description fields as measured by tf-idf.  Note that in many fields we have an "n" of 1 and a "tf" of 1 meaning that these were descrption finds that only had a single word in them.  If a document only has one word, the tf-idf algorithm wil think that it is a very important word.

## Connecting description fields to keywords

Now let's plot some of hte most important words, as measured by tf-idf, for a few example keywords used on NASA datasets.  First, we will use dplyr operations to filter for keywords we want to examine and look at the top 15 words for each keyword:


```{r}
desc_tf_idf <- full_join(desc_tf_idf, nasa_keyword, by = "id")


desc_tf_idf %>%
  filter(!near(tf, 1)) %>%
  filter(keyword %in% c("SOLAR ACTIVITY", "CLOUDS", "NASA HEADQUARTERS",
                        "ASTROPHYSICS", "GEOGRAPHY", "BUDGET")) %>%
  arrange(desc(tf_idf)) %>%
  group_by(keyword) %>%
  distinct(word, keyword, .keep_all = TRUE) %>%
  top_n(15, tf_idf) %>%
  ungroup() %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>%
  ggplot(aes(word, tf_idf, fill = keyword)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~keyword, ncol = 3, scales = "free") +
  coord_flip() +
  labs(title = "Highest tf-idf words in NASA metadata description fields",
       caption = "NASA metadata rom https://data.nasa.gov/data.json",
       x = NULL, y = "tf-idf")

```

These graphs show us good validity for the tf-idf scores we have developed.  The budget one shows us financial information, OMB, and years.  Geography shows thing we would expect like mapping, maps, and geoogical.  This would help us discover datasets we might want to consider (or searches for datasets).


# Topic Modeling

The tf-idf scores did give us some insight into the content of the different description fields, but we can try another approach to find out what hte description fields are about.  We can use topic modeling to model each document (description field) as a mixture of topics and each topic as a mixtrue of words.  We are doing to latent Dirichlet allocation (LDA), even though there are multiple options for this.

## Casting to a Document-Term Matrix (DTM)

We need to turn our tidy data into a document term matrix using the `cast_dtm()` function from tidytext package.  Rows in this format correspond to doucments and columns correstond to terms (words); it is a sparse matri and the values are word counts.

Befor we do this though, we should clean up the text a bit to stop using some nonsense words leftover from HTML or other character encoding.  We are doing to use `bind_rows()` to corm a custom stop words list and then do an anti_join again.

```{r}
custom_stop_word <- bind_rows(stop_words,
                           data_frame(word = c("nbsp", "amp", "gt", "lt",
                                               "timesnewromanpsmt", "font",
                                               "td", "li", "br", "tr", "quot",
                                               "st", "img", "src", "strong",
                                               "http", "file", "files",
                                               as.character(1:12)),
                                      lexicon = rep("custom", 30)))
word_counts <- nasa_description %>%
  anti_join(my_stop_words) %>%
  count(id, word, sort = TRUE) %>%
  ungroup()

word_counts
```

This is the information we need, the number of times each word is used in each document, now we can `cast()` from out tidy text into the dtm format.

```{r}
desc_dtm <- word_counts %>%
  cast_dtm(id, word, n)

desc_dtm
```

We can see that we have the correct number of documents, that this is pretty much 100% spare, meaning that almost all of the entries in this matrix are zero.  Each non-zero entry corresponds to a cetain word appearing in a certain document.


### Ready for Topic Modeling

Now we can use the topimodels package to create the LDA model.  We don't know ahead of time how many topics we want, just like k-means clustering.  The authors tried a number of topics (8, 16, 24, 32, and 64), but found that 24 topics is the best.  AT 24 the documents are getting sorted into topics cleanly, but going much beyond that caused the distributions of $\gamma$, the probability that eacch document belongs in each topic, to look worrisome (something to be discussed later).

```{r}
library(topicmodels)

# this will take a long time to run this model (over 30 minutes now):
desc_lda <- LDA(desc_dtm, k = 24, control = list(seed = 1234))
```

The seed control is only here to get same results as the authors.  This is because the stochastic algorithm could have different results depending on where the algorithm started.  Thus we had to specify a `seed`.



### Interpreting the topic model

Now that we have the analysis, lets use the `tidy()` function of the tidytext apcakge to build a tidy data frame that summarises the results of the model.

```{r}
tidy_lda <- tidy(desc_lda)

tidy_lda
```


The column $\beta$ tells us the probability  of that term being generated from that specific topic for that document.  Some of extremely low and others are not that low.  Let's look at each topic.  Let's examine the top 10 terms for each topic:

```{r}
top_terms <- tidy_lda %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms
```


This is too large, so it might be better to visualize this information.

```{r}
top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  group_by(topic, term) %>%    
  arrange(desc(beta)) %>%  
  ungroup() %>%
  mutate(term = factor(paste(term, topic, sep = "__"), 
                       levels = rev(paste(term, topic, sep = "__")))) %>%
  ggplot(aes(term, beta, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
  labs(title = "Top 10 terms in each LDA topic",
       x = NULL, y = expression(beta)) +
  facet_wrap(~ topic, ncol = 4, scales = "free")
```

This is a very large graph, which you will need to expand to get a better view.  We can see that "data" appears frequently in these, but that is unsurprising because all of these are datasets. You will see, however, that there are some pretty good grouping for the different topics (some about thermal, some about propulsion, water, etc).  

Now that we have looked at which words are associated with which topics.  Next, let's examine which topics are association with which description fields (i.e. documents).  We will look at the $\gamma$ probability, which is the probaiblity that each document belongs in each topic.  This is also done using the `tidy()` verb.

```{r}
lda_gamma <- tidy(desc_lda, matrix = "gamma")

lda_gamma
```


Notice that some of hte probaiblities visible at the top of the data frame are low and some are higher.  Our model has assigned a probability to each description belonging to each of the topics we constructed from the set of words.  How are the probabilities distributed?

```{r}
ggplot(lda_gamma, aes(gamma)) +
  geom_histogram() +
  scale_y_log10() +
  labs(title = "Distribution of probabilities for all topics",
       y = "Number of documents", x = expression(gamma))

```

First, note that the y-axis is plotted on a log scale, otherwise it is difficult to make out any detail in the plot.  Next, note that $\gamma$ runs from 0 to 1 and that this is the probability that a given document belongs in a given topic.  There are many values near zero, which means there are many documents that do not belong in each topic.  Also, there are a number of values near 1; these are the documents that do belong in each topic.  This distribution shows that documents are being well discriminated as belonging to a topic or not.  We can also look at how the probabilities are distributed within each topic, as we will show below.  


```{r}
ggplot(lda_gamma, aes(gamma, fill = as.factor(topic))) +
  geom_histogram(show.legend = FALSE) +
  facet_wrap(~topic, ncol = 4) +
  scale_y_log10() +
  labs(title = "Distribution of probability for each topic",
       y = "Number of Documents", x = expression(gamma))
```


Some topics do better than others in classifying the models.  Topic 15 and 21 looks pretty good.  A $\gamma$ of 0 meanas that the document has basically no chance of being in that topic, while a probability of 1 meaning that it is completely in there.  As we can see, there are a lot of documents that fall in the middle.  

The chart above help in deciding the number of topics (clusters) to have in an LDA model.  At this one, I would think about doing fewer topics.


## Connecting Topic Modeling with Keywords

Now let's do a `full_join()` of the topic models and the keywords to see what relationships we can find.  
```{r}
lda_gamma <- full_join(lda_gamma, nasa_keyword, by = c("document" = "id"))

lda_gamma
```



Now we can use `filter()` to keep only teh document-topic entries that have probabilities greater thansome cut-off value (we will use 0.9).

```{r}
top_keywords <- lda_gamma %>%
  filter(gamma > 0.9) %>%
  count(topic, keyword, sort = TRUE)

top_keywords
```



From here we can look at top keywords for each topic:

```{r}
top_keywords %>%
  group_by(topic) %>%
  top_n(5) %>%
  group_by(topic, keyword) %>%
  arrange(desc(n)) %>%
  ungroup() %>%
  mutate(keyword = factor(paste(keyword, topic, sep = "__"),
                          levels = rev(paste(keyword, topic, sep = "__")))) %>%
  ggplot(aes(keyword, n, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  labs(title = "Top Keywords of each LDA Topic",
       x = NULL, y = "Number of Documents") +
  coord_flip() +
  scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
  facet_wrap(~topic, ncol = 4, scales = "free")
```


This plot tells us "for the datasets with descriptions fields that have a high probability of belonging to a given topic, which are hte most common human-assigned keywords?"





