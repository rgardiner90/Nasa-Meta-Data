---
title: "Nasa Meta Data"
author: "Richard G. Gardiner"
date: "12/5/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Intro

Disclosure: Most of the information below comes from *Text Mining with R* by David Robinson.  

NASA has tens of thousands of datasets that cover everything from earth science to aeospace enginerring.  We can use metadata for these datasets to understand how they all connect.  The metadata includes the name of the dataset, description, which organization(s) is responsible for the dataset, and keywords.  The metadata is publically available only in JSON format.

Using techniques like tf-idf and topic modeling, we will explose the connections between the different datasets.

# How the data is organized

The first we want to do is download the JSON file and look at the names of what is stroed in teh metadata

```{r, cache = TRUE}
library(jsonlite)

metadata <- fromJSON("https://data.nasa.gov/data.json")
names(metadata$dataset)
```


We see here that we could extract a lot of information from who publishes each dataset to what license they are released under.  It seems likely that the title, description, and keywords for each dataset may be the best for connecting the datasets.  Let's check them out:

```{r}
class(metadata$dataset$title)
class(metadata$dataset$description)
class(metadata$dataset$keyword)
```

The title and description are stored as characters while the keyword is stored as a list.

## Wrangling and Tidying the data

Now we can set up tidy data frames for title, description, and keyword, keeping the data ids for each so we can connect them if necessary for later analysis

```{r}
library(tidyverse)

nasa_title <- data_frame(id = metadata$dataset$identifier, 
                         title = metadata$dataset$title)

unique <- unique(nasa_title$id)
```


The book has a different way to capture the id, but it wasn't working and this is the best idea I had for this.  It appears that there may be a little bit of overlap, but that is going to have to do for now (the book also mentions overlap).


```{r}
nasa_description <- data_frame(id = metadata$dataset$identifier,
                               desc = metadata$dataset$description)

nasa_description %>%
  select(desc) %>%
  sample_n(5)
```

Lastly, we will want to build a tidy data frame for the keywords.  For this one, we need to use `unnest()` from tidyr, because they are in a list-column.

```{r}
nasa_keyword <- data_frame(id = metadata$dataset$identifier,
                          keyword = metadata$dataset$keyword) %>%
  unnest(keyword)

nasa_keyword
```

This is a tidy data frame where each row is a keyword.  This means there are multiple rows for most of the datasets.

Now we can use the `unnest_tokens()` function for the title and description fiedls so we can start doing text analysis.  We will also remove stop from the descriptions and titles, but not the keywords because they are short-human assigned keywords.

```{r}
library(tidytext)

nasa_title <- nasa_title %>%
  unnest_tokens(word, title) %>%
  anti_join(stop_words)

nasa_description <- nasa_description %>%
  unnest_tokens(word, desc) %>%
  anti_join(stop_words)
```


Now they are in a tidy text format with one token (word) per row:

```{r}
nasa_title
```


```{r}
nasa_description
```


## Initial Analysis


Let's get a list of the most common words in the dataset titles using dplyr.

```{r}
nasa_title %>%
  count(word, sort = TRUE)
```

How about descriptions?

```{r}
nasa_description %>%
  count(word, sort = TRUE)
```

Let's remove some of the words that are used frequently, but are not meaningful to most of us.  This can be done using a custom stop word then doing an anti join.

```{r}
my_stop_words <- data_frame(word = c(as.character(1:5),
                                 "ii", "v1.0", "l2", "l3", "1"))

nasa_title <- nasa_title %>%
  anti_join(my_stop_words)

nasa_description <- nasa_description %>%
  anti_join(my_stop_words)
```

What are the most common keywords?

```{r}
nasa_keyword %>%
  count(keyword, sort = TRUE)
```


This looks pretty good, but we might want to change all of our text to upper or lower case to get rid of duplicates when we do analysis:

```{r}
nasa_keyword <- nasa_keyword %>%
  mutate(keyword = toupper(keyword))

```

# Word co-ocurrences and correlations

Lets examine which words commonly occur together in the title,s descriptions, and keywords of NASA datasets.  This may help us see which datasets are related to each other.

## Networks of description and title words

We can use `pairwise_count()` from widyr package to count how many time each pair of words occur together in a title or description field.

```{r}
library(widyr)

title_word_pairs <- nasa_title %>%
  widyr::pairwise_count(word, id, sort = TRUE, upper = FALSE) 

title_word_pairs
```

These are pairs of words that occur together most often in title fields.  We see a lot of "phase" in this top 10.  Now let's do the same with descriptions:

```{r}
desc_word_pairs <- nasa_description %>% 
  widyr::pairwise_count(word, id, sort = TRUE, upper = FALSE)

desc_word_pairs
```

As with titles, the word "phase" and "system" are pretty common.


Let's plot networks of these cooccurring words so we can see these relationships better.  We will use the ggraph package to visualize the networks:

```{r}
library(ggplot2)
library(igraph)
library(ggraph)


set.seed(1234)
title_word_pairs %>%
  filter(n >= 200) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
  geom_node_point(size = 3) +
  geom_node_text(aes(label = name), repel = TRUE,
                 point.padding = unit(0.2, "lines")) +
  theme_void()

```

So phase clearly is the key part of this.  Perhaps it is actually too important, but we can deal with that later.  It might be more helpful to do tf-idf as a metric to find characteristic words for each description field.  Let's look at the description fields:

```{r}
set.seed(1234)
desc_word_pairs %>%
  filter(n >= 1200) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "blue") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE,
                 point.padding = unit(0.2, "lines")) +
  theme_void()
```


This is showing a more complex network (though I had to play with the filter).

Now let's do networks of keywords:

```{r}
keyword_pairs <- nasa_keyword %>%
  pairwise_count(keyword, id, sort = TRUE, upper = FALSE)

keyword_pairs
```


```{r}
set.seed(1234)
keyword_pairs %>%
  filter(n >= 200) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "red") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE,
                 point.padding = unit(0.2, "lines")) +
  theme_void()

```


NOTE: these are the most common keywords occuring together, but they are also in general the most common keywords.

We can also do pairwise correlation with keywords to see which keywords that are more likely to occur together than other keywords.

```{r}
keyword_cor <- nasa_keyword %>%
  group_by(keyword) %>%
  filter(n() >= 50) %>%
  pairwise_cor(keyword, id, sort = TRUE, upper = FALSE)

keyword_cor
```

Note that we have a few that always occur together (correlation of 1).  This means that they are redundant keywords.  We can visualize a network of keyword correlations just like the co-occurences.

```{r}
set.seed(1234)
keyword_cor %>%
  filter(correlation > .7) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation, edge_width = correlation), edge_colour = "green") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE,
                 point.padding = unit(0.2, "lines")) +
  theme_void()

```

This is rather interesting and different than the co=occurences network.  The main difference is that this graph is asking about which keyword pairs occur more oten together than with other words. In this network, there are a lot of clusters that are independent of each other.  This can be extracted for further analysis.



# Calculating tf-idf for the description fields

The results of the pairwise count for the descriptions showed that there are just a few words that dominate the fields.  In this instance, using tf-idf would be great to find words that are espeically important to a document within a collection of documents.  For this example, we will consider each description field a "document" and the entire dataframe as the colelction (corpus) of documents.  Since we have already used the `unnest_tokens()` function, we only need to use the `bind_tf_idf()` function:

```{r}
desc_tf_idf <- nasa_description %>%
  count(id, word, sort = TRUE) %>%
  ungroup() %>%
  bind_tf_idf(word, id, n)

```


```{r}
desc_tf_idf %>%
  arrange(-tf_idf)

```

These are teh mostimportant words in teh description fields as measured by tf-idf.  Note that in many fields we have an "n" of 1 and a "tf" of 1 meaning that these were descrption finds that only had a single word in them.  If a document only has one word, the tf-idf algorithm wil think that it is a very important word.

## Connecting description fields to keywords

Now let's plot some of hte most important words, as measured by tf-idf, for a few example keywords used on NASA datasets.  First, we will use dplyr operations to filter for keywords we want to examine and look at the top 15 words for each keyword:


```{r}
desc_tf_idf <- full_join(desc_tf_idf, nasa_keyword, by = "id")


desc_tf_idf %>%
  filter(!near(tf, 1)) %>%
  filter(keyword %in% c("SOLAR ACTIVITY", "CLOUDS", "NASA HEADQUARTERS",
                        "ASTROPHYSICS", "GEOGRAPHY", "BUDGET")) %>%
  arrange(desc(tf_idf)) %>%
  group_by(keyword) %>%
  distinct(word, keyword, .keep_all = TRUE) %>%
  top_n(15, tf_idf) %>%
  ungroup() %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>%
  ggplot(aes(word, tf_idf, fill = keyword)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~keyword, ncol = 3, scales = "free") +
  coord_flip() +
  labs(title = "Highest tf-idf words in NASA metadata description fields",
       caption = "NASA metadata rom https://data.nasa.gov/data.json",
       x = NULL, y = "tf-idf")

```

These graphs show us good validity for the tf-idf scores we have developed.  The budget one shows us financial information, OMB, and years.  Geography shows thing we would expect like mapping, maps, and geoogical.  This would help us discover datasets we might want to consider (or searches for datasets).
